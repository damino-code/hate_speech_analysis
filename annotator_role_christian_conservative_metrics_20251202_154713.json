{
  "timestamp": "20251202_154713",
  "analysis_file": "annotator_role_christian_conservative_20251202_143031.csv",
  "analysis_type": "annotator_role_christian_conservative",
  "metrics": {
    "sentiment": {
      "annotator_role_christian_conservative_accuracy": 0.22,
      "annotator_role_christian_conservative_f1_macro": 0.09166666666666666,
      "annotator_role_christian_conservative_f1_weighted": 0.09533333333333333,
      "annotator_role_christian_conservative_mae": 0.9676982722112184,
      "n_samples": 50,
      "n_classes": 5
    },
    "respect": {
      "annotator_role_christian_conservative_accuracy": 0.86,
      "annotator_role_christian_conservative_f1_macro": 0.46236559139784944,
      "annotator_role_christian_conservative_f1_weighted": 0.795268817204301,
      "annotator_role_christian_conservative_mae": 1.354351318432685,
      "n_samples": 50,
      "n_classes": 2
    },
    "insult": {
      "annotator_role_christian_conservative_accuracy": 0.2,
      "annotator_role_christian_conservative_f1_macro": 0.08620689655172414,
      "annotator_role_christian_conservative_f1_weighted": 0.06896551724137931,
      "annotator_role_christian_conservative_mae": 1.845073110595405,
      "n_samples": 50,
      "n_classes": 4
    },
    "humiliate": {
      "annotator_role_christian_conservative_accuracy": 0.24,
      "annotator_role_christian_conservative_f1_macro": 0.18489984591679506,
      "annotator_role_christian_conservative_f1_weighted": 0.11476117103235747,
      "annotator_role_christian_conservative_mae": 1.6523766690281971,
      "n_samples": 50,
      "n_classes": 3
    },
    "status": {
      "annotator_role_christian_conservative_accuracy": 1.0,
      "annotator_role_christian_conservative_f1_macro": 1.0,
      "annotator_role_christian_conservative_f1_weighted": 1.0,
      "annotator_role_christian_conservative_mae": 0.7304895254137449,
      "n_samples": 50,
      "n_classes": 2
    },
    "dehumanize": {
      "annotator_role_christian_conservative_accuracy": 0.42,
      "annotator_role_christian_conservative_f1_macro": 0.3441881501582994,
      "annotator_role_christian_conservative_f1_weighted": 0.29067390321121667,
      "annotator_role_christian_conservative_mae": 1.1586004255298192,
      "n_samples": 50,
      "n_classes": 2
    },
    "violence": {
      "annotator_role_christian_conservative_accuracy": 0.76,
      "annotator_role_christian_conservative_f1_macro": 0.4318181818181818,
      "annotator_role_christian_conservative_f1_weighted": 0.6736363636363636,
      "annotator_role_christian_conservative_mae": 0.9000008156943877,
      "n_samples": 50,
      "n_classes": 2
    },
    "genocide": {
      "annotator_role_christian_conservative_accuracy": 0.88,
      "annotator_role_christian_conservative_f1_macro": 0.46808510638297873,
      "annotator_role_christian_conservative_f1_weighted": 0.8425531914893617,
      "annotator_role_christian_conservative_mae": 0.8450548187277204,
      "n_samples": 50,
      "n_classes": 2
    },
    "attack_defend": {
      "annotator_role_christian_conservative_accuracy": 0.38,
      "annotator_role_christian_conservative_f1_macro": 0.13768115942028986,
      "annotator_role_christian_conservative_f1_weighted": 0.22028985507246376,
      "annotator_role_christian_conservative_mae": 0.7357784813476637,
      "n_samples": 50,
      "n_classes": 4
    }
  }
}